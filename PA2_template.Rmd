---
title: "Reproducible Research: Peer Assessment 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Title: JHU-DSCI Reproducible Research
## Peer Assessment 2: NOAA Storm Data

### Name: Walter Yu
### Date: July 2020

## Synopsis

This document completes an initial analysis of the NOAA Storm Data set as follows: 

1. Download and read the raw NOAA storm dataset 
2. Review and clean data before analysis 
3. Analyze data to answer assignment questions 
4. Plot data to communicate results 
5. Document steps and findings to be reproducible

## Submission Notes

1. This markdown file is an analysis and visualization of the NOAA weather dataset. Source data available [here][1].

[1]: https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2

2. The document for this assignment was generated with [R Studio Cloud][2] due to package installation errors on my local machine; as a result, file was run and report rendered there instead.

[2]: https://rstudio.cloud

This markdown project template is based on a fork of the course assignment Github repo available [here][3].

[3]: https://github.com/rdpeng/RepData_PeerAssessment1

This assignment is completed for the JHU Coursera Data Science Program, which is a 10-course certification. More info about this program is available [here][4].

[4]: https://www.coursera.org/specializations/jhu-data-science

## Questions

1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

2. Across the United States, which types of events have the greatest economic consequences?

## Part 1: Data Processing

### Part 1A: Data Import

Steps:

1. Initially attempted to load data from url
2. However, attempts resulted in error so loaded from bz2 zipfile instead
3. From there, file was unzipped and read into program
4. Data import step is saved as its own chunk and [cached][1a_cache]

[1a_cache]: https://bookdown.org/yihui/rmarkdown-cookbook/cache.html

### Part 1B: Data Clean

Steps:

1. Used na.omit function to remove null values
2. However, na.omit removed most values do not used on dataset

Analysis:

1. There are 37 variables, only some of which are needed for analysis
2. NOAA documentation quantifies health impacts with fatailities/injuries 
3. So, aggregate fatailities/injuries by event type

<!-- cache may crash rstudio cloud -->
# ```{r echo=TRUE, cache=TRUE}

```{r echo=TRUE}

# part 1a: data import

# source: https://rpubs.com/otienodominic/398952
# note: unsuccessful after several attempts, so revert to reading from bz2 file
# url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
# temp <- tempfile()
# download.file(url, temp, mode="wb")
# data <- read.csv("repdata_data_StormData.csv.bz2", header=TRUE, sep=",")
# unlink(zipfile)

# source: assignment instructions
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/38y35MMiEeiERhLphT2-QA
# data <- read.csv("data/repdata_data_StormData.csv.bz2")

```

<!-- cache may crash rstudio cloud -->
# ```{r echo=TRUE, cache=TRUE}

```{r echo=TRUE}

# part 1b: data processing
# note: course discussion forum consulted for these steps:
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/38y35MMiEeiERhLphT2-QA

# review dataset
# conclusion: dataset contains many columns, only some of which seem to be useful
# https://www.statmethods.net/stats/descriptives.html
# head(data)
names(data)
summary(data)

# attempt remove null values
# source: https://www.statmethods.net/input/missingdata.html
# data_omit <- na.omit(data)

# verify results
# conclusion: na.omit removes most records, so not used on dataset
# https://www.statmethods.net/stats/descriptives.html
# head(data_omit)
# names(data_omit)
# summary(data_omit)

# identify event types
# unique(data$EVTYPE)A

# todo: agrepl usage
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/XdHXZccTEemRWg4GTNs16g

# calculate mean/median by date
# aggregate by fatality or injury, then apply calculations
# source: https://www.statmethods.net/management/aggregate.html
event_fatal_total <- aggregate(FATALITIES ~ EVTYPE, data, sum)

# rank aggregate results:
# https://stackoverflow.com/questions/23659241/rank-in-the-aggregate-function
event_fatal_total <- event_fatal_total[order(event_fatal_total$FATALITIES, decreasing=TRUE),]
head(event_fatal_total, n=30L)

# create barplots
# install.packages("ggplot2")
# library(ggplot2)
# Basic barplot
p <- ggplot(data=event_fatal_total, aes(x=EVTYPE, y=FATALITIES)) +
  geom_bar(stat="identity")
p + coord_flip()

```

