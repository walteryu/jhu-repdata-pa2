---
title: "Reproducible Research: Peer Assessment 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Title: JHU-DSCI Reproducible Research
## Peer Assessment 2: NOAA Storm Data

### Name: Walter Yu
### Date: July 2020

## Synopsis

This document completes an initial analysis of the NOAA Storm Data set as follows: 

1. Download and read the raw NOAA storm dataset 
2. Review and clean data before analysis 
3. Analyze data to answer assignment questions 
4. Plot data to communicate results 
5. Document steps and findings to be reproducible

## Submission Notes

1. This markdown file is an analysis and visualization of the NOAA weather dataset. Source data available [here][1].

[1]: https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2

2. The document for this assignment was generated with [R Studio Cloud][2] due to package installation errors on my local machine; as a result, file was run and report rendered there instead.

[2]: https://rstudio.cloud

This markdown project template is based on a fork of the course assignment Github repo available [here][3].

[3]: https://github.com/rdpeng/RepData_PeerAssessment1

This assignment is completed for the JHU Coursera Data Science Program, which is a 10-course certification. More info about this program is available [here][4].

[4]: https://www.coursera.org/specializations/jhu-data-science

## Questions

1. Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

2. Across the United States, which types of events have the greatest economic consequences?

## Part 1: Data Processing

### Part 1A: Data Import

Steps:

1. Initially attempted to load data from url
2. However, attempts resulted in error so loaded from bz2 zipfile instead
3. From there, file was unzipped and read into program
4. Data import step is saved as its own chunk and [cached][1a_cache]

[1a_cache]: https://bookdown.org/yihui/rmarkdown-cookbook/cache.html

### Part 1B: Data Clean

Steps:

1. Used na.omit function to remove null values
2. However, na.omit removed most values do not used on dataset

Analysis:

1. There are 37 variables, only some of which are needed for analysis
2. NOAA documentation quantifies health impacts with fatailities/injuries 
3. So, aggregate fatailities/injuries by event type

```{r echo=TRUE}

# part 1a: data import read from csv.bz2 format

# source: https://rpubs.com/otienodominic/398952
# note: unsuccessful after several attempts, so revert to reading from bz2 file
# url <- "https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
# temp <- tempfile()
# download.file(url, temp, mode="wb")
# data <- read.csv("repdata_data_StormData.csv.bz2", header=TRUE, sep=",")
# unlink(zipfile)

# source: assignment instructions
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/38y35MMiEeiERhLphT2-QA
data <- read.csv("data/repdata_data_StormData.csv.bz2")

```

```{r echo=TRUE}

# part 1b: data processing - review data

# note: course discussion forum consulted for these steps:
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/38y35MMiEeiERhLphT2-QA

# review dataset
# conclusion: dataset contains many columns, only some of which seem to be useful
# https://www.statmethods.net/stats/descriptives.html
# head(data)
# names(data)
# summary(data)

# attempt remove null values
# source: https://www.statmethods.net/input/missingdata.html
# data_omit <- na.omit(data)

# verify results
# conclusion: na.omit removes most records, so not used on dataset
# https://www.statmethods.net/stats/descriptives.html
# head(data_omit)
# names(data_omit)
# summary(data_omit)

# identify event types
# conclusion: results show a large number of event types
# unique(data$EVTYPE)

# todo: agrepl usage
# https://www.coursera.org/learn/reproducible-research/discussions/weeks/4/threads/XdHXZccTEemRWg4GTNs16g

```

```{r echo=TRUE}

# part 1b: data processing - analyze event types by fatalities

# aggregate by fatality or injury, then apply calculations
# source: https://www.statmethods.net/management/aggregate.html
event_fatal_total <- aggregate(FATALITIES ~ EVTYPE, data, sum)

# rank aggregate results:
# https://stackoverflow.com/questions/23659241/rank-in-the-aggregate-function
event_fatal_total <- event_fatal_total[order(event_fatal_total$FATALITIES, decreasing=TRUE),]

# subset for top 50 event types with highest count
# https://stackoverflow.com/questions/2667673/select-first-4-rows-of-a-data-frame-in-r/47400307
event_fatal_50 <- event_fatal_total[1:50,]
# event_fatal_50

# ratio of fatalities due to top 50 event types / all event types
event_fatal_50_sum <- sum(event_fatal_50$FATALITIES)
event_fatal_total_sum <- sum(event_fatal_total$FATALITIES)
event_fatal_ratio <- event_fatal_50_sum / event_fatal_total_sum
print("ratio of fatalities from top 50 / all event types:")
print(event_fatal_ratio)

# note: based on results above, storm-related and heat have highest totals 
# conclusion: calculate ratios for these events to create plots
event_fatal_8 <- event_fatal_total[1:8,]
event_fatal_8

```

```{r echo=TRUE}

# part 1b: data processing - analyze event types by injuries

# aggregate by fatality or injury, then apply calculations
# source: https://www.statmethods.net/management/aggregate.html
event_injury_total <- aggregate(INJURIES ~ EVTYPE, data, sum)

# rank aggregate results:
# https://stackoverflow.com/questions/23659241/rank-in-the-aggregate-function
event_injury_total <- event_injury_total[order(event_injury_total$INJURIES, decreasing=TRUE),]

# subset for top 50 event types with highest count
# https://stackoverflow.com/questions/2667673/select-first-4-rows-of-a-data-frame-in-r/47400307
event_injury_50 <- event_injury_total[1:50,]
# event_injury_50

# ratio of injuryities due to top 50 event types / all event types
event_injury_50_sum <- sum(event_injury_50$INJURIES)
event_injury_total_sum <- sum(event_injury_total$INJURIES)
event_injury_ratio <- event_injury_50_sum / event_injury_total_sum
print("ratio of injuries from top 50 / all event types:")
print(event_injury_ratio)

# note: based on results above, storm-related and heat have highest totals 
# conclusion: calculate ratios for these events to create plots
event_injury_6 <- event_injury_total[1:6,]
event_injury_6

```

```{r echo=TRUE}

# part 1b: data processing - plot event types by fatalities

# note: based on results above, storm-related and heat have highest totals 
# conclusion: calculate ratios for these events to create plots
# http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization
# https://stackoverflow.com/questions/16961921/plot-data-in-descending-order-as-appears-in-data-frame
# install.packages("ggplot2")
# library(ggplot2)
p_fatal <- ggplot(data=event_fatal_8, aes(x=reorder(EVTYPE, -FATALITIES), y=FATALITIES, fill=EVTYPE)) + geom_bar(stat="identity")
p_fatal + coord_flip()

```

```{r echo=TRUE}

# part 1b: data processing - plot event types by injuries

# note: based on results above, storm-related and heat have highest totals 
# conclusion: calculate ratios for these events to create plots
# http://www.sthda.com/english/wiki/ggplot2-barplots-quick-start-guide-r-software-and-data-visualization
# https://stackoverflow.com/questions/16961921/plot-data-in-descending-order-as-appears-in-data-frame
# install.packages("ggplot2")
# library(ggplot2)
p_injury <- ggplot(data=event_injury_6, aes(x=reorder(EVTYPE, -INJURIES), y=INJURIES, fill=EVTYPE)) + geom_bar(stat="identity")
p_injury + coord_flip()

```
